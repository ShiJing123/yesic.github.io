<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[CVPR 2018]A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)]]></title>
    <url>%2F2018%2F06%2F19%2FCVPR-2018-A-Neural-Multi-sequence-Alignment-TeCHnique-NeuMATCH%2F</url>
    <content type="text"><![CDATA[论文提出了一种端到端训练的模型，称为NeuMATCH，用于处理多个异源（即对齐的两个目标的来源不同，例如视频和文本的对齐）序列对齐的问题。 Introduction序列对齐主要包含三种类型：一对一，一对多，非单调。 传统的序列对齐方法包括两个阶段：1）学习两个句子的相似性度量；2）找到最优的对齐路径。例如DTW，CTW。 基于DTW的方法属于马尔科夫假设，只考虑了局部的上下文信息，然而有利于对齐的上下文信息可能分散在整个序列，例如一些叙述性的电影。 文中采用的是视频和文本对齐的数据，尤其是包含一些叙述性的内容，例如电影。选择叙述性的内容原因在于，由于事件之间存在大量的因果和时间相互作用，是计算理解中最具挑战性的。 Contributions 1）论文提出了一种端到端的循环框架，用于异源端个序列对齐问题。不像之前的方法，我们在决策时考虑了更多的上下文信息；2）标注了一个新的数据集 ，包含了电影摘要视频。 Approach任务：给定两个序列，视频序列和文本序列。视频序列包含了一系列连续的镜头（video clips），记为$\mathcal{V}=_{i=1,2…,N}$ 。文本序列包含了一系列连续的句子，记为${\mathcal{S}}=_{i,2,…,M}$ 。任务是找到一个函数$\pi$将视频段的下标映射到相应的句子：$\langle V_i,S_i \rangle$ 。 由于需要学习相似性度量的上下文信息分散在整个序列上，所以在决策时需要考虑过去和未来的信息。 Architecture整个模型由四个LSTM组成，包含了输入视频序列（Video Stack），输入文本序列（Text Stack），先前对齐动作（Action Stack），先前对齐匹配（Match Stack）。 这里的stack不是堆叠的意思，而是数据结构中的stack，先入先出。这也是本文的一个亮点。 Language and Visual Encoders先对每个镜头（video clips）和每个句子编码，然后在通过可选择的预训练，将编码后对的镜头和句子嵌入到同一个空间。通过预训练可以得到很好的初始化。 Video Encoder 利用VGG-16训练镜头的每一帧，将第一层全连接层的特征提取出来，为4096维。对所有帧的特征进行mean pooling，作为每个镜头的特征。镜头特征在经过三层全连接层，最后的到video的编码信息$v_i$，表示第i个镜头（clip）的编码信息。 Sentence Encoder 对每个词使用GloVe嵌入，在经过两层LSTM后，得到最后一层隐藏层的特征$h_t$，在经过三层全连接层得到每个句子的编码信息$s_i$。 **Enc The NeuMATCH Alignment NetworkLSTM Stacks Video Stack 对于每一时刻$t$，video stack包含了还没处理的序列$V_t,V_{t+1},…,V_N$ 。LSTM的方向是从$V_N$到$V_t$，允许信息从未来的clip流到当前的clip，将这个LSTM作为video stack，其隐藏层特征为$h_t^V$。 sentence stack 处理方法和video stack一样， 对于每一时刻$t$，sentence stack包含了还没处理的序列$S_t,S_{t+1},…,S_N$，其隐藏层特征为$h_t^S$。 action stack action stack负责储存过去所有的对齐动作，动作极为$A_{t-1},…,A_1$，每个动作为one-hot编码。动作的信息流不同于video stack和sentence stack，信息是从第一个动作流向最后一个动作，其隐藏层状态记为$h_{t-1}^A$。 文中定义了五种基本的对齐动作，它们一起处理两个序列对齐的问题，包含了不匹配（例如，一个clip没有句子和它匹配，记为null）以及一对多匹配（一个句子和多个clip匹配）。 五个对齐动作分别为：Pop Clip (PC),Pop Sentence (PS), Match (M), Match-Retain Clip (MRC), and Match-Retain Sentence (MRS)。下面是一个操作例子。 需要注意的是，并不是这五个动作都需要一起使用，具体问题具体分析，例如一对一的对齐，每个句子和某个clip匹配，允许某个clip和没有句子匹配，则只需要Pop Clip以及Match；一对多的对齐，允许每个句子和多个clip匹配，允许某个clip和没有句子匹配，每个句子至少和一个clip匹配，则只需要Pop Clip, Pop Sentence, and Match- Retain Sentence。 matched stack match stack包含了先前已经匹配的clip和sentence，其中最后匹配的置于stack的顶部。本文考虑一个句子可以匹配多个clip。因为匹配的clip在内容上是相似的，所以一个句子匹配多个clip的，取它们的均值，即$v_i=\sum_j^Kv_j/K$。则LSTM的输入为sentence和clip的拼接，$r_i=[s_i,v_I]$ 。最后一个隐藏层状态为$h_{t-1}^M$。 Alignment Action Prediction 每一时刻$t$，四个stack的状态为$\Psi_t=(V_{t^+},S_{t^+},A_{(t-1)^-},R_{1^+})$ 。则第$t$时刻$A_t$的条件概率为$$P(A_t|\Psi_t)=P(A_t|V_{t^+},S_{t^+},A_{(t-1)^-},R_{1^+})$$上面的计算方式为$\psi_t$经过两层全连接层然后在softmax。 根据链式法则，$$P(A_1,…,A_N|\mathcal{V},\mathcal{S})=\prod_{t=1}^NP(A_t|A_{(t-1)^-},\Psi_t)$$将交叉熵作为损失函数。 Experimental Evaluationdataset HM-1为在序列中随机插入一些不同电影的片段，但是具有相似的情节，HM-2随机删除一些句子，YMS是论文提出的new dataset，数来源于youtube。 baselines Minimum Distance (MD), Dynamic Time Warping (DTW) Canonical Time Warping (CTW) results 一对一 一对多]]></content>
      <categories>
        <category>paper notes</category>
        <category>CVPR</category>
        <category>2018</category>
      </categories>
      <tags>
        <tag>sequence alignment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>test</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[NIPS 2017]Learning Hierarchical Information Flow with Recurrent Neural Modules]]></title>
    <url>%2F2018%2F06%2F10%2FNIPS-2017-Learning-Hierarchical-Information-Flow-with-Recurrent-Neural-Modules%2F</url>
    <content type="text"><![CDATA[论文受到新脑皮质（neocortical）通信方式的启发，提出了ThalNet。新脑皮质通信方式有两种，一种是直连，另一种是通过丘脑（thalamus）。论文受到第二种通信方式的启发，构建多个循环神经模块，将所有模块的特征发送到一个路由中心，使得模块在多个时间步能够共享特征。模型展示了模块对输入数据的链式处理，学习了层次化的路由信息，并且模型包含了前馈神经网络、跳跃连接、反馈连接等结构。 模型结构 论文构建的模型包括四个模块，模块$f^1$接受input， $f^2$，$f^3$为侧模块，$f^4$负责输出，每个模块$f$可以是全连接层、GRU、LSTM等。对于每一时刻t，每个模块将其特征$\phi_t$发给路由中心 $\Phi$，其中每个模块特征由上下文$c_t$和一个可以选择的输入 $x_t$决定。对于下一时刻的上下文 $c_{t+1}$，每个模块通过阅读机制（read mechanism）选择性地读取路由中心$\Phi$的特征。对于每个模块的特征计算，上下文计算，路由中心，模型输出式子见下图。 阅读机制包括两种：静态阅读，动态阅读。静态阅读只取决于 $\Phi$，即$r^i(\Phi)$，而动态阅读不仅取决于 $\Phi$，还取决于当前模块的特征，即 $r^i(\Phi,\phi^i)$。 文中给出了四种阅读方法，其中weight normalization、Fast softmax比较稳定，效果比较好。 实验论文在三个任务上做了实验， Sequential Permuted MNIST、Sequential CIFAR-10、Text8 Language Modeling。其中前两个是图像的延迟预测，即将图像的像素每一行看作一个时刻，在输入最后一个时刻时预测出图像的类别，第三个任务是输入每个character，预测下一个character。]]></content>
      <categories>
        <category>paper notes</category>
        <category>NIPS</category>
        <category>2017</category>
      </categories>
      <tags>
        <tag>hierarchical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[ICLR 2018]Hierarchical Representations for Efficient Architecture Search]]></title>
    <url>%2F2018%2F06%2F10%2FICLR-2018-Hierarchical-Representations-for-Efficient-Architecture-Search%2F</url>
    <content type="text"><![CDATA[论文结合了一种新型层次化遗传表示体系（hierarchical genetic representation scheme），可以模仿人类专家常用的模块化设计模式，支持复杂的拓扑结构。分别在CIFAR-10和ImageNet上获得了top-1错误率为3.6%和20.3%。 在这项工作中，作者通过强加一个层次化的网络来限制搜索空间结构，允许每层是一些灵活的网络拓扑结构（有向无环图）。底层是一些基本操作，例如卷积，池化等，更高层的计算图，或者modif，是由低层modif作为它们的构建块来组成，最高层的modif通过堆叠形成最终的网络结构。如下图所示。 作者通过进化搜索或者随机搜索来发现层次化结构。基于遗传算法，作者定义了一些变异（mutation），每次选择某一层的某个motif的某条边发生变异，这可以使得modif的结构发生改变（删除，添加，修改），将验证集的准确率作为fitness。 在实验中，提出的搜索框架只学习cell的结构，而不是整个模型。原因是这样能够快速计算fitness，然后可以将对应的genotype转移到大的模型，也就是用更少的cell计算fitness，更多的cell评估模型。下图是使用框架搜索优化过的cell构建的图像分类模型。]]></content>
      <categories>
        <category>paper notes</category>
        <category>ICLR</category>
        <category>2018</category>
      </categories>
      <tags>
        <tag>hierarchical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[IJCAI 2018]A Hierarchical End-to-End Model for Jointly Improving Text Summarization and Sentiment Classification]]></title>
    <url>%2F2018%2F06%2F10%2FIJCAI-2018-A-Hierarchical-End-to-End-Model-for-Jointly-Improving-Text-Summarization-and-Sentiment-Classification%2F</url>
    <content type="text"><![CDATA[论文提出了一种层次化端到端模型，整合了文本摘要生成和情感分类任务 。 模型借鉴来源：Hierarchical Attention Networks for Document Classification 任务给定样本$(x^i,y^i,l^i)$，包含了文本，摘要，情感标签，同时进行摘要生成和情感分类。其中$L_i$为文本的单词数， $M_i$为摘要的单词数。 模型结构模型包括三部分：text encoder，summary decoder，sentiment classifier text encoder 使用双向LSTM作为encoder，产生上下文信息$h={h_1,h_2,…,h_L}$ summary decoderSummary decoder包括三部分：单向LSTM，multi-view attention，word generator 利用单向LSTM产生decoder的隐藏层输出$s_t$ 利用multi-view attention生成摘要向量$\mathbf{v}^{(c)}$和情感向量$\mathbf{v}^{(t)}$，multi-view attention其实就是两个独立的global attention。摘要向量$\mathbf{v}^{(c)}$生成如下： 根据摘要向量 $\mathbf{v}^{(c)}$，利用生成摘要中的每个词。 sentiment classifier 将所有时间步的情感向量 $\mathbf{v}^{(t)}$收集起来 为了获得原文本的上下文信息，使用highway机制，将上下文信息h作为分类器输入的一部分，r为情感分类的输入向量 实验作者在亚马逊在线评论数据集上做了实验，在和其他模型的对比上均获得了最好的结果。]]></content>
      <categories>
        <category>paper notes</category>
        <category>IJCAI</category>
        <category>2018</category>
      </categories>
      <tags>
        <tag>encoder decoder</tag>
        <tag>text summarization</tag>
        <tag>sentiment classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[NAACL 2016]Hierarchical Attention Networks for Document Classification]]></title>
    <url>%2F2018%2F06%2F10%2FNAACL-2016-Hierarchical-Attention-Networks-for-Document-Classification%2F</url>
    <content type="text"><![CDATA[论文提出了一种层次化的注意力网络用于文本分类，模型有两个特点：1）它有一个层次化的结构，反映了文本的层次化结构（由词组成句；由句组成文本）；2）它有两个level的注意力机制，分别是word-level和sentence-level。 模型结构模型由四部分组成：word encoder，word attention，sentence encoder，sentence attention word encoder 对每个词进行word embedding，在通过双向GRU提取隐藏层状态，将两个方向的隐藏层状态拼接， $w_{it}$为文本第i个句子的第t个词 。 word attention 对每个句子的所有词做attention，然后计算每个句子向量$s_i$： sentence encoder 和word encoder方式一样，利用双向GRU提取每个句子向量$s_i$的信息，然后将两个方向的隐藏层状态拼接 sentence attention和word attention方式一样，对文本的每个句子做attention，然后计算文本向量v，将v用softmax分类。 实验作者在6个大规模文本分类数据集上做了实验，和众多SVM，CNN，LSTM比较均获得了最好的结果。]]></content>
      <categories>
        <category>paper notes</category>
        <category>NAACL</category>
        <category>2016</category>
      </categories>
      <tags>
        <tag>hierarchical</tag>
        <tag>encoder decoder</tag>
        <tag>attention</tag>
        <tag>document classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[arxiv 2018]Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction]]></title>
    <url>%2F2018%2F06%2F09%2Farxiv-2018-Hierarchical-Attention-Based-Recurrent-Highway-Networks-for-Time-Series-Prediction%2F</url>
    <content type="text"><![CDATA[论文提出了一种可以端到端学习的深度学习模型，Hierarchical Attention-Based Recurrent Highway Networks(HARNN)，可以将外部序列的时空特征提取和目标序列的时空动态建模结合到一个简单的框架。 论文模型借鉴来源：A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction 模型贡献 利用卷积网络学习外部输入（x）中变量之间的空间特征。接着利用RHN（recurrent highway network）在不同层构建不同的语义，建模时序动态。 提出层次化注意力机制。 在获得高准确率的同时，可以捕获时间序列中的突然振荡或者变动。 任务时间序列预测，给定1~T-1时刻的目标序列以及1~T-1时刻的外部序列，预测T时刻的目标值。 模型结构encoder和decoder均使用多层RHN，在encoder使用卷积网络提取外部序列的时空特征，在decoder使用层次化注意力机制对时序依赖动态建模。 encoder 将外部输入$(\mathbf{x_1,x_2,…,x_{T-1}})$通过几层卷积、池化操作后输给一个全连接层，得到一个特征向量 $(\mathbf{w_1,w_2,…,w_{T-1}})$ 利用RHN对encoder-1中的特征向量进行时序动态建模。$\mathbf{h}_t^{[k]}$表示第k层第t时刻的隐藏层状态 decoder 多层注意力机制：在t时刻，对encoder每一层的1~T-1时刻的隐藏层状态做attention。其中$\mathbf{s_{t-1}}=\mathbf{s}_{t-1}^{[k]}$ ，t-1时刻decoder第k层隐藏层状态。 则第k层第t时刻的子上下文$\mathbf{d}_t^{[k]}$为 将t时刻所有层的子上下文拼接，得到t时刻的上下文$\mathbf{d}_t$： 更新decoder输入$\mathbf{y}_t$为$\mathbf{\tilde{y}_t}$： 解码器RHN第k层隐藏层状态$\mathbf{s}_{t}^{[k]}$： 第T时刻预测值$\mathbf{\hat{y}_T}$： 实验作者在三个数据集上和ARIMA，LSTM，GRU，DA-RNN模型比较，均获得最好的结果。]]></content>
      <categories>
        <category>paper notes</category>
        <category>arxiv</category>
        <category>2018</category>
      </categories>
      <tags>
        <tag>hierarchical</tag>
        <tag>time series prediction</tag>
        <tag>encoder decoder</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[IJCAI 2017]A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction]]></title>
    <url>%2F2018%2F06%2F09%2FIJCAI-2017-A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction%2F</url>
    <content type="text"><![CDATA[论文提出了基于注意力机制的两阶段循环神经网络（DA-RNN），在第一阶段（encoder），引入input attention mechanism对每一时刻的外部输入自适应性地提取相关性；在第二阶段（decoder），引入temporal attention mechanism捕获encoder的长期时序依赖信息。 任务时间序列预测，给定1~T-1时刻的目标序列以及1~T-1时刻的外部序列，预测T时刻的目标值。 模型结构 encoder 使用LSTM作为encoder和decoder，其中状态更新如下 ： 引入input attention mechanism：对每一时刻$\mathbf{X_t}$的n维变量使用attention 使用$\alpha_t^k$对$\mathbf{X_t}$加权求和，更新$\mathbf{X_t}$： 使用$\tilde{X_t}$更新1中的状态方程： decoder 引入temporal attention mechanism：在decoder的第t时刻，对encoder所有隐藏层状态做attention 计算t时刻的上下文向量$\mathbf{c_t}$： 利用$\mathbf{c_t}$更新目标序列的输入值$\mathbf{y_{t-1}}$为$\mathbf{\tilde{y}_{t-1}}$： 更新第t时刻decoder的隐藏层状态$\mathbf{d_t}$： 其中$f_2$的更新状态方程如encoder-1： 则第t时刻的预测值$\mathbf{\hat{T}_t}$为： 实验作者在两个数据集上和ARIMA，NARX RNN，Encoder-Decoder，Attention RNN模型进行了比较，均获得了最好的结果。]]></content>
      <categories>
        <category>paper notes</category>
        <category>IJCAI</category>
        <category>2017</category>
      </categories>
      <tags>
        <tag>hierarchical</tag>
        <tag>time series prediction</tag>
        <tag>encoder decoder</tag>
        <tag>attention</tag>
      </tags>
  </entry>
</search>
