<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tech-Port</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-06-20T10:40:35.628Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>yesic</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[CVPR 2018]A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)</title>
    <link href="http://yoursite.com/2018/06/19/CVPR-2018-A-Neural-Multi-sequence-Alignment-TeCHnique-NeuMATCH/"/>
    <id>http://yoursite.com/2018/06/19/CVPR-2018-A-Neural-Multi-sequence-Alignment-TeCHnique-NeuMATCH/</id>
    <published>2018-06-19T15:42:10.000Z</published>
    <updated>2018-06-20T10:40:35.628Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了一种端到端训练的模型，称为NeuMATCH，用于处理多个异源（即对齐的两个目标的来源不同，例如视频和文本的对齐）序列对齐的问题。</p></blockquote><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>序列对齐主要包含三种类型：一对一，一对多，非单调。</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/aeCh1DLEf9.png?imageslim" alt="mark"></p><p>传统的序列对齐方法包括两个阶段：1）学习两个句子的相似性度量；2）找到最优的对齐路径。例如DTW，CTW。</p><p>基于DTW的方法属于马尔科夫假设，只考虑了局部的上下文信息，然而有利于对齐的上下文信息可能分散在整个序列，例如一些叙述性的电影。</p><p>文中采用的是视频和文本对齐的数据，尤其是包含一些叙述性的内容，例如电影。选择叙述性的内容原因在于，由于事件之间存在大量的因果和时间相互作用，是计算理解中最具挑战性的。</p><p><strong>Contributions</strong>  1）论文提出了一种端到端的循环框架，用于异源端个序列对齐问题。不像之前的方法，我们在决策时考虑了更多的上下文信息；2）标注了一个新的<a href="https://github.com/pelindogan/NeuMATCH" target="_blank" rel="noopener">数据集</a> ，包含了电影摘要视频。</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p><strong>任务</strong>：给定两个序列，视频序列和文本序列。视频序列包含了一系列连续的镜头（video clips），记为$\mathcal{V}=_{i=1,2…,N}$ 。文本序列包含了一系列连续的句子，记为${\mathcal{S}}=_{i,2,…,M}$ 。任务是找到一个函数$\pi$将视频段的下标映射到相应的句子：$\langle V_i,S_i \rangle$ 。</p><p><strong><u>由于需要学习相似性度量的上下文信息分散在整个序列上，所以在决策时需要考虑过去和未来的信息。</u></strong> </p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>整个模型由四个LSTM组成，包含了输入视频序列（Video Stack），输入文本序列（Text Stack），先前对齐动作（Action Stack），先前对齐匹配（Match Stack）。</p><p><strong><u>这里的stack不是堆叠的意思，而是数据结构中的stack，先入先出。这也是本文的一个亮点。</u></strong></p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/mKc05J5ag4.png?imageslim" alt="mark">  </p><h4 id="Language-and-Visual-Encoders"><a href="#Language-and-Visual-Encoders" class="headerlink" title="Language and Visual Encoders"></a><em>Language and Visual Encoders</em></h4><p>先对每个镜头（video clips）和每个句子编码，然后在通过可选择的预训练，将编码后对的镜头和句子嵌入到同一个空间。通过预训练可以得到很好的初始化。</p><p><strong>Video Encoder</strong>  利用VGG-16训练镜头的每一帧，将第一层全连接层的特征提取出来，为4096维。对所有帧的特征进行mean pooling，作为每个镜头的特征。镜头特征在经过三层全连接层，最后的到video的编码信息$v_i$，表示第i个镜头（clip）的编码信息。</p><p><strong>Sentence Encoder</strong>   对每个词使用GloVe嵌入，在经过两层LSTM后，得到最后一层隐藏层的特征$h_t$，在经过三层全连接层得到每个句子的编码信息$s_i$。</p><p>**Enc</p><h4 id="The-NeuMATCH-Alignment-Network"><a href="#The-NeuMATCH-Alignment-Network" class="headerlink" title="The NeuMATCH Alignment Network"></a><em>The NeuMATCH Alignment Network</em></h4><p><strong>LSTM Stacks</strong> </p><ul><li><p><em>Video Stack</em>   对于每一时刻$t$，video stack包含了还没处理的序列$V_t,V_{t+1},…,V_N$ 。LSTM的方向是从$V_N$到$V_t$，允许信息从未来的clip流到当前的clip，将这个LSTM作为video stack，其隐藏层特征为$h_t^V$。</p></li><li><p><em>sentence stack</em>  处理方法和video stack一样， 对于每一时刻$t$，sentence stack包含了还没处理的序列$S_t,S_{t+1},…,S_N$，其隐藏层特征为$h_t^S$。</p></li><li><p><em>action stack</em>   action stack负责储存过去所有的对齐动作，动作极为$A_{t-1},…,A_1$，每个动作为one-hot编码。动作的信息流不同于video stack和sentence stack，信息是从第一个动作流向最后一个动作，其隐藏层状态记为$h_{t-1}^A$。</p><p>文中定义了五种基本的对齐动作，它们一起处理两个序列对齐的问题，包含了不匹配（例如，一个clip没有句子和它匹配，记为null）以及一对多匹配（一个句子和多个clip匹配）。</p><p>五个对齐动作分别为：Pop Clip (PC),Pop Sentence (PS), Match (M), Match-Retain Clip (MRC), and Match-Retain Sentence (MRS)。下面是一个操作例子。</p><p><strong><u>需要注意的是，并不是这五个动作都需要一起使用，具体问题具体分析，例如一对一的对齐，每个句子和某个clip匹配，允许某个clip和没有句子匹配，则只需要Pop Clip以及Match；一对多的对齐，允许每个句子和多个clip匹配，允许某个clip和没有句子匹配，每个句子至少和一个clip匹配，则只需要Pop Clip, Pop Sentence, and Match- Retain Sentence。</u></strong></p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/5bF8KCldig.png?imageslim" alt="mark"></p></li></ul><ul><li><em>matched stack</em>   match stack包含了先前已经匹配的clip和sentence，其中最后匹配的置于stack的顶部。本文考虑一个句子可以匹配多个clip。因为匹配的clip在内容上是相似的，所以一个句子匹配多个clip的，取它们的均值，即$v_i=\sum_j^Kv_j/K$。则LSTM的输入为sentence和clip的拼接，$r_i=[s_i,v_I]$ 。最后一个隐藏层状态为$h_{t-1}^M$。</li></ul><p><strong>Alignment Action Prediction</strong></p><p>每一时刻$t$，四个stack的状态为$\Psi_t=(V_{t^+},S_{t^+},A_{(t-1)^-},R_{1^+})$ 。则第$t$时刻$A_t$的条件概率为<br>$$<br>P(A_t|\Psi_t)=P(A_t|V_{t^+},S_{t^+},A_{(t-1)^-},R_{1^+})<br>$$<br>上面的计算方式为$\psi_t$经过两层全连接层然后在softmax。</p><p>根据链式法则，<br>$$<br>P(A_1,…,A_N|\mathcal{V},\mathcal{S})=\prod_{t=1}^NP(A_t|A_{(t-1)^-},\Psi_t)<br>$$<br>将交叉熵作为损失函数。</p><h3 id="Experimental-Evaluation"><a href="#Experimental-Evaluation" class="headerlink" title="Experimental Evaluation"></a>Experimental Evaluation</h3><p><strong>dataset</strong></p><p>HM-1为在序列中随机插入一些不同电影的片段，但是具有相似的情节，HM-2随机删除一些句子，YMS是论文提出的new dataset，数来源于youtube。</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/I4giJf559F.png?imageslim" alt="mark"></p><p><strong>baselines</strong></p><ul><li>Minimum Distance (MD),</li><li>Dynamic Time Warping (DTW)</li><li>Canonical Time Warping (CTW)</li></ul><p><strong>results</strong></p><ul><li><p>一对一</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/ECgI7i0E11.png?imageslim" alt="mark"></p></li><li><p>一对多</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/7e4FiEmA4i.png?imageslim" alt="mark"></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了一种端到端训练的模型，称为NeuMATCH，用于处理多个异源（即对齐的两个目标的来源不同，例如视频和文本的对齐）序列对齐的问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="CVPR" scheme="http://yoursite.com/categories/paper-notes/CVPR/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/CVPR/2018/"/>
    
    
      <category term="sequence alignment" scheme="http://yoursite.com/tags/sequence-alignment/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/06/11/hello-world/"/>
    <id>http://yoursite.com/2018/06/11/hello-world/</id>
    <published>2018-06-10T17:09:11.897Z</published>
    <updated>2018-06-09T17:33:29.441Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="test" scheme="http://yoursite.com/categories/test/"/>
    
    
  </entry>
  
  <entry>
    <title>[NIPS 2017]Learning Hierarchical Information Flow with Recurrent Neural Modules</title>
    <link href="http://yoursite.com/2018/06/10/NIPS-2017-Learning-Hierarchical-Information-Flow-with-Recurrent-Neural-Modules/"/>
    <id>http://yoursite.com/2018/06/10/NIPS-2017-Learning-Hierarchical-Information-Flow-with-Recurrent-Neural-Modules/</id>
    <published>2018-06-09T17:35:44.000Z</published>
    <updated>2018-06-09T17:50:18.698Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文受到新脑皮质（neocortical）通信方式的启发，提出了ThalNet。新脑皮质通信方式有两种，一种是直连，另一种是通过丘脑（thalamus）。论文受到第二种通信方式的启发，构建多个循环神经模块，将所有模块的特征发送到一个路由中心，使得模块在多个时间步能够共享特征。模型展示了模块对输入数据的链式处理，学习了层次化的路由信息，并且模型包含了前馈神经网络、跳跃连接、反馈连接等结构。</p></blockquote><a id="more"></a><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="https://github.com/yesic/pic/blob/master/hif/hif-1.jpg?raw=true" alt=""></p><p>论文构建的模型包括四个模块，模块$f^1$接受input， $f^2$，$f^3$为侧模块，$f^4$负责输出，每个模块$f$可以是全连接层、GRU、LSTM等。对于每一时刻t，每个模块将其特征$\phi_t$发给路由中心 $\Phi$，其中每个模块特征由上下文$c_t$和一个可以选择的输入 $x_t$决定。对于下一时刻的上下文 $c_{t+1}$，每个模块通过阅读机制（read mechanism）选择性地读取路由中心$\Phi$的特征。对于每个模块的特征计算，上下文计算，路由中心，模型输出式子见下图。</p><p><img src="https://github.com/yesic/pic/blob/master/hif/hif-2.jpg?raw=true" alt=""></p><p>阅读机制包括两种：静态阅读，动态阅读。静态阅读只取决于 $\Phi$，即$r^i(\Phi)$，而动态阅读不仅取决于 $\Phi$，还取决于当前模块的特征，即 $r^i(\Phi,\phi^i)$。</p><p>文中给出了四种阅读方法，其中weight normalization、Fast softmax比较稳定，效果比较好。 </p><p><img src="https://github.com/yesic/pic/blob/master/hif/hif-3.jpg?raw=true" alt=""><img src="https://github.com/yesic/pic/blob/master/hif/hif-4.jpg?raw=true" alt=""></p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>论文在三个任务上做了实验， Sequential Permuted MNIST、Sequential CIFAR-10、Text8 Language Modeling。其中前两个是图像的延迟预测，即将图像的像素每一行看作一个时刻，在输入最后一个时刻时预测出图像的类别，第三个任务是输入每个character，预测下一个character。 </p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文受到新脑皮质（neocortical）通信方式的启发，提出了ThalNet。新脑皮质通信方式有两种，一种是直连，另一种是通过丘脑（thalamus）。论文受到第二种通信方式的启发，构建多个循环神经模块，将所有模块的特征发送到一个路由中心，使得模块在多个时间步能够共享特征。模型展示了模块对输入数据的链式处理，学习了层次化的路由信息，并且模型包含了前馈神经网络、跳跃连接、反馈连接等结构。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="NIPS" scheme="http://yoursite.com/categories/paper-notes/NIPS/"/>
    
      <category term="2017" scheme="http://yoursite.com/categories/paper-notes/NIPS/2017/"/>
    
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
  </entry>
  
  <entry>
    <title>[ICLR 2018]Hierarchical Representations for Efficient Architecture Search</title>
    <link href="http://yoursite.com/2018/06/10/ICLR-2018-Hierarchical-Representations-for-Efficient-Architecture-Search/"/>
    <id>http://yoursite.com/2018/06/10/ICLR-2018-Hierarchical-Representations-for-Efficient-Architecture-Search/</id>
    <published>2018-06-09T17:24:00.000Z</published>
    <updated>2018-06-09T17:50:34.382Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文结合了一种新型层次化遗传表示体系（hierarchical genetic representation scheme），可以模仿人类专家常用的模块化设计模式，支持复杂的拓扑结构。分别在CIFAR-10和ImageNet上获得了top-1错误率为3.6%和20.3%。 </p></blockquote><a id="more"></a><p>在这项工作中，作者通过强加一个层次化的网络来限制搜索空间结构，允许每层是一些灵活的网络拓扑结构（有向无环图）。底层是一些基本操作，例如卷积，池化等，更高层的计算图，或者modif，是由低层modif作为它们的构建块来组成，最高层的modif通过堆叠形成最终的网络结构。如下图所示。</p><p><img src="https://github.com/yesic/pic/blob/master/hgrs/hgrs-1.jpg?raw=true" alt=""></p><p>作者通过进化搜索或者随机搜索来发现层次化结构。基于遗传算法，作者定义了一些变异（mutation），每次选择某一层的某个motif的某条边发生变异，这可以使得modif的结构发生改变（删除，添加，修改），将验证集的准确率作为fitness。</p><p>在实验中，提出的搜索框架只学习cell的结构，而不是整个模型。原因是这样能够快速计算fitness，然后可以将对应的genotype转移到大的模型，也就是用更少的cell计算fitness，更多的cell评估模型。下图是使用框架搜索优化过的cell构建的图像分类模型。</p><p><img src="https://github.com/yesic/pic/blob/master/hgrs/hgrs-2.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文结合了一种新型层次化遗传表示体系（hierarchical genetic representation scheme），可以模仿人类专家常用的模块化设计模式，支持复杂的拓扑结构。分别在CIFAR-10和ImageNet上获得了top-1错误率为3.6%和20.3%。 &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="ICLR" scheme="http://yoursite.com/categories/paper-notes/ICLR/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/ICLR/2018/"/>
    
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
  </entry>
  
  <entry>
    <title>[IJCAI 2018]A Hierarchical End-to-End Model for Jointly Improving Text Summarization and Sentiment Classification</title>
    <link href="http://yoursite.com/2018/06/10/IJCAI-2018-A-Hierarchical-End-to-End-Model-for-Jointly-Improving-Text-Summarization-and-Sentiment-Classification/"/>
    <id>http://yoursite.com/2018/06/10/IJCAI-2018-A-Hierarchical-End-to-End-Model-for-Jointly-Improving-Text-Summarization-and-Sentiment-Classification/</id>
    <published>2018-06-09T16:44:10.000Z</published>
    <updated>2018-06-09T18:01:48.677Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了一种层次化端到端模型，整合了文本摘要生成和情感分类任务 。</p></blockquote><a id="more"></a><blockquote><p>模型借鉴来源：<a href="https://yesic.github.io/2018/06/10/NAACL-2016-Hierarchical-Attention-Networks-for-Document-Classification/" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a></p></blockquote><hr><h4 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h4><p>给定样本$(x^i,y^i,l^i)$，包含了文本，摘要，情感标签，同时进行摘要生成和情感分类。其中$L_i$为文本的单词数， $M_i$为摘要的单词数。</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-1.jpg?raw=true" alt=""><br><br></div><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>模型包括三部分：text encoder，summary decoder，sentiment classifier</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-2.jpg?raw=true" alt=""><br><br></div><h5 id="text-encoder"><a href="#text-encoder" class="headerlink" title="text encoder"></a>text encoder</h5><ol><li>使用双向LSTM作为encoder，产生上下文信息$h={h_1,h_2,…,h_L}$</li></ol><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-3.jpg?raw=true" alt=""></div><h5 id="summary-decoder"><a href="#summary-decoder" class="headerlink" title="summary decoder"></a>summary decoder</h5><p>Summary decoder包括三部分：单向LSTM，multi-view attention，word generator</p><ol><li><p>利用单向LSTM产生decoder的隐藏层输出$s_t$</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-4.jpg?raw=true" alt=""></div></li><li><p>利用multi-view attention生成摘要向量$\mathbf{v}^{(c)}$和情感向量$\mathbf{v}^{(t)}$，multi-view attention其实就是两个独立的global attention。摘要向量$\mathbf{v}^{(c)}$生成如下：</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-5.jpg?raw=true" alt=""></div></li><li><p>根据摘要向量 $\mathbf{v}^{(c)}$，利用生成摘要中的每个词。 </p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-6.jpg?raw=true" alt=""></div></li></ol><h5 id="sentiment-classifier"><a href="#sentiment-classifier" class="headerlink" title="sentiment classifier"></a>sentiment classifier</h5><ol><li><p>将所有时间步的情感向量 $\mathbf{v}^{(t)}$收集起来</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-7.jpg?raw=true" alt=""></div></li><li><p>为了获得原文本的上下文信息，使用highway机制，将上下文信息h作为分类器输入的一部分，r为情感分类的输入向量</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-8.jpg?raw=true" alt=""></div></li></ol><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>作者在亚马逊在线评论数据集上做了实验，在和其他模型的对比上均获得了最好的结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了一种层次化端到端模型，整合了文本摘要生成和情感分类任务 。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="IJCAI" scheme="http://yoursite.com/categories/paper-notes/IJCAI/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/IJCAI/2018/"/>
    
    
      <category term="encoder decoder" scheme="http://yoursite.com/tags/encoder-decoder/"/>
    
      <category term="text summarization" scheme="http://yoursite.com/tags/text-summarization/"/>
    
      <category term="sentiment classification" scheme="http://yoursite.com/tags/sentiment-classification/"/>
    
  </entry>
  
  <entry>
    <title>[NAACL 2016]Hierarchical Attention Networks for Document Classification</title>
    <link href="http://yoursite.com/2018/06/10/NAACL-2016-Hierarchical-Attention-Networks-for-Document-Classification/"/>
    <id>http://yoursite.com/2018/06/10/NAACL-2016-Hierarchical-Attention-Networks-for-Document-Classification/</id>
    <published>2018-06-09T16:19:35.000Z</published>
    <updated>2018-06-09T17:19:33.854Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了一种层次化的注意力网络用于文本分类，模型有两个特点：1）它有一个层次化的结构，反映了文本的层次化结构（由词组成句；由句组成文本）；2）它有两个level的注意力机制，分别是word-level和sentence-level。 </p></blockquote><a id="more"></a><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>模型由四部分组成：word encoder，word attention，sentence encoder，sentence attention</p><p><img src="https://github.com/yesic/pic/blob/master/han/han-1.jpg?raw=true" alt=""></p><h5 id="word-encoder"><a href="#word-encoder" class="headerlink" title="word encoder"></a>word encoder</h5><ol><li><p>对每个词进行word embedding，在通过双向GRU提取隐藏层状态，将两个方向的隐藏层状态拼接， $w_{it}$为文本第i个句子的第t个词 。</p><p><img src="https://github.com/yesic/pic/blob/master/han/han-2.jpg?raw=true" alt=""></p></li></ol><p><img src="https://github.com/yesic/pic/blob/master/han/han-3.jpg?raw=true" alt=""></p><h5 id="word-attention"><a href="#word-attention" class="headerlink" title="word attention"></a>word attention</h5><ol><li>对每个句子的所有词做attention，然后计算每个句子向量$s_i$：</li></ol><p><img src="https://github.com/yesic/pic/blob/master/han/han-4.jpg?raw=true" alt=""></p><h5 id="sentence-encoder"><a href="#sentence-encoder" class="headerlink" title="sentence encoder"></a>sentence encoder</h5><ol><li><p>和word encoder方式一样，利用双向GRU提取每个句子向量$s_i$的信息，然后将两个方向的隐藏层状态拼接</p><p><img src="https://github.com/yesic/pic/blob/master/han/han-5.jpg?raw=true" alt=""></p><p><img src="https://github.com/yesic/pic/blob/master/han/han-6.jpg?raw=true" alt=""></p></li></ol><h5 id="sentence-attention"><a href="#sentence-attention" class="headerlink" title="sentence attention"></a>sentence attention</h5><p>和word attention方式一样，对文本的每个句子做attention，然后计算文本向量v，将v用softmax分类。</p><p><img src="https://github.com/yesic/pic/blob/master/han/han-7.jpg?raw=true" alt=""></p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>作者在6个大规模文本分类数据集上做了实验，和众多SVM，CNN，LSTM比较均获得了最好的结果。 </p><p><img src="https://github.com/yesic/pic/blob/master/han/han-8.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了一种层次化的注意力网络用于文本分类，模型有两个特点：1）它有一个层次化的结构，反映了文本的层次化结构（由词组成句；由句组成文本）；2）它有两个level的注意力机制，分别是word-level和sentence-level。 &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="NAACL" scheme="http://yoursite.com/categories/paper-notes/NAACL/"/>
    
      <category term="2016" scheme="http://yoursite.com/categories/paper-notes/NAACL/2016/"/>
    
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
      <category term="encoder decoder" scheme="http://yoursite.com/tags/encoder-decoder/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="document classification" scheme="http://yoursite.com/tags/document-classification/"/>
    
  </entry>
  
  <entry>
    <title>[arxiv 2018]Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction</title>
    <link href="http://yoursite.com/2018/06/09/arxiv-2018-Hierarchical-Attention-Based-Recurrent-Highway-Networks-for-Time-Series-Prediction/"/>
    <id>http://yoursite.com/2018/06/09/arxiv-2018-Hierarchical-Attention-Based-Recurrent-Highway-Networks-for-Time-Series-Prediction/</id>
    <published>2018-06-09T14:21:15.000Z</published>
    <updated>2018-06-09T17:19:27.084Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了一种可以端到端学习的深度学习模型，Hierarchical Attention-Based Recurrent Highway Networks(HARNN)，可以将外部序列的时空特征提取和目标序列的时空动态建模结合到一个简单的框架。</p></blockquote><a id="more"></a><blockquote><p>论文模型借鉴来源：<a href="https://yesic.github.io/2018/06/09/IJCAI-2017-A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction/" target="_blank" rel="noopener">A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction</a></p></blockquote><hr><h4 id="模型贡献"><a href="#模型贡献" class="headerlink" title="模型贡献"></a>模型贡献</h4><ul><li>利用卷积网络学习外部输入（<strong>x</strong>）中变量之间的空间特征。接着利用RHN（recurrent highway network）在不同层构建不同的语义，建模时序动态。</li><li>提出层次化注意力机制。</li><li>在获得高准确率的同时，可以捕获时间序列中的突然振荡或者变动。</li></ul><h4 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h4><p>时间序列预测，给定<code>1~T-1</code>时刻的目标序列以及<code>1~T-1</code>时刻的外部序列，预测T时刻的目标值。</p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>encoder和decoder均使用多层RHN，在encoder使用卷积网络提取外部序列的时空特征，在decoder使用层次化注意力机制对时序依赖动态建模。</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-1.jpg?raw=true" alt=""></p><h5 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h5><ol><li><p>将外部输入$(\mathbf{x_1,x_2,…,x_{T-1}})$通过几层卷积、池化操作后输给一个全连接层，得到一个特征向量 $(\mathbf{w_1,w_2,…,w_{T-1}})$ </p></li><li><p>利用RHN对<code>encoder-1</code>中的特征向量进行时序动态建模。$\mathbf{h}_t^{[k]}$表示第k层第t时刻的隐藏层状态 </p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-2.jpg?raw=true" alt=""></p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-3.jpg?raw=true" alt=""></p></li></ol><h5 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h5><ol><li><p>多层注意力机制：在t时刻，对encoder每一层的1~T-1时刻的隐藏层状态做attention。其中$\mathbf{s_{t-1}}=\mathbf{s}_{t-1}^{[k]}$ ，t-1时刻decoder第k层隐藏层状态。</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-4.jpg?raw=true" alt=""></p></li><li><p>则第k层第t时刻的子上下文$\mathbf{d}_t^{[k]}$为</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-5.jpg?raw=true" alt=""></p><p>将t时刻所有层的子上下文拼接，得到t时刻的上下文$\mathbf{d}_t$：</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-6.jpg?raw=true" alt=""></p></li><li><p>更新decoder输入$\mathbf{y}_t$为$\mathbf{\tilde{y}_t}$：</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-7.jpg?raw=true" alt=""></p></li><li><p>解码器RHN第k层隐藏层状态$\mathbf{s}_{t}^{[k]}$：</p><p> <img src="https://github.com/yesic/pic/blob/master/harnn/harnn-8.jpg?raw=true" alt=""></p></li><li><p>第T时刻预测值$\mathbf{\hat{y}_T}$：</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-9.jpg?raw=true" alt=""></p></li></ol><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>作者在三个数据集上和ARIMA，LSTM，GRU，DA-RNN模型比较，均获得最好的结果。 </p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-10.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了一种可以端到端学习的深度学习模型，Hierarchical Attention-Based Recurrent Highway Networks(HARNN)，可以将外部序列的时空特征提取和目标序列的时空动态建模结合到一个简单的框架。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="arxiv" scheme="http://yoursite.com/categories/paper-notes/arxiv/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/arxiv/2018/"/>
    
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
      <category term="time series prediction" scheme="http://yoursite.com/tags/time-series-prediction/"/>
    
      <category term="encoder decoder" scheme="http://yoursite.com/tags/encoder-decoder/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>[IJCAI 2017]A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction</title>
    <link href="http://yoursite.com/2018/06/09/IJCAI-2017-A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction/"/>
    <id>http://yoursite.com/2018/06/09/IJCAI-2017-A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction/</id>
    <published>2018-06-09T10:46:59.000Z</published>
    <updated>2018-06-09T17:19:21.344Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了基于注意力机制的两阶段循环神经网络（DA-RNN），在第一阶段（encoder），引入input attention mechanism对每一时刻的外部输入自适应性地提取相关性；在第二阶段（decoder），引入temporal attention mechanism捕获encoder的长期时序依赖信息。</p></blockquote><a id="more"></a><h4 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h4><p>时间序列预测，给定<code>1~T-1</code>时刻的目标序列以及<code>1~T-1</code>时刻的外部序列，预测T时刻的目标值。</p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-1.jpg?raw=true" alt="DA-RNN"></p><h5 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h5><ol><li><p>使用LSTM作为encoder和decoder，其中状态更新如下 ：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-2.jpg?raw=true" alt=""></p></li><li><p>引入input attention mechanism：对每一时刻$\mathbf{X_t}$的n维变量使用attention</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-3.jpg?raw=true" alt=""> </p></li><li><p>使用$\alpha_t^k$对$\mathbf{X_t}$加权求和，更新$\mathbf{X_t}$：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-4.jpg?raw=true" alt=""></p></li><li><p>使用$\tilde{X_t}$更新<code>1</code>中的状态方程：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-5.jpg?raw=true" alt=""></p></li></ol><h5 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h5><ol><li><p>引入temporal attention mechanism：在decoder的第t时刻，对encoder所有隐藏层状态做attention </p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-6.jpg?raw=true" alt=""></p></li><li><p>计算t时刻的上下文向量$\mathbf{c_t}$：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-8.jpg?raw=true" alt=""></p></li><li><p>利用$\mathbf{c_t}$更新目标序列的输入值$\mathbf{y_{t-1}}$为$\mathbf{\tilde{y}_{t-1}}$：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-7.jpg?raw=true" alt=""></p></li><li><p>更新第t时刻decoder的隐藏层状态$\mathbf{d_t}$：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-9.jpg?raw=true" alt=""></p><p>其中$f_2$的更新状态方程如<code>encoder-1</code>：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-10.jpg?raw=true" alt=""></p></li></ol><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-11.jpg?raw=true" alt=""></p><ol><li><p>则第t时刻的预测值$\mathbf{\hat{T}_t}$为：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-12.jpg?raw=true" alt=""></p></li></ol><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>作者在两个数据集上和ARIMA，NARX RNN，Encoder-Decoder，Attention RNN模型进行了比较，均获得了最好的结果。 </p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-13.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了基于注意力机制的两阶段循环神经网络（DA-RNN），在第一阶段（encoder），引入input attention mechanism对每一时刻的外部输入自适应性地提取相关性；在第二阶段（decoder），引入temporal attention mechanism捕获encoder的长期时序依赖信息。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="IJCAI" scheme="http://yoursite.com/categories/paper-notes/IJCAI/"/>
    
      <category term="2017" scheme="http://yoursite.com/categories/paper-notes/IJCAI/2017/"/>
    
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
      <category term="time series prediction" scheme="http://yoursite.com/tags/time-series-prediction/"/>
    
      <category term="encoder decoder" scheme="http://yoursite.com/tags/encoder-decoder/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
</feed>
